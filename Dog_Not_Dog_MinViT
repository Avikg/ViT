{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4969890,"sourceType":"datasetVersion","datasetId":2882514},{"sourceId":257181,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":219835,"modelId":241594}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Define dataset paths\ndataset_path = \"/kaggle/input/dog-vs-not-dog/Dog vs Not-Dog\"\ncsv_path = os.path.join(dataset_path, \"labels.csv\")\n\n# Load the CSV file\ndf = pd.read_csv(csv_path)\n\n# Create a new list for valid entries\nvalid_entries = []\nfor _, row in df.iterrows():\n    img_name = row[\"filename\"]\n    label = row[\"label\"]\n    \n    # Construct the expected file path\n    img_path = os.path.join(dataset_path, \"dog\" if label == \"dog\" else \"other\", img_name)\n\n    # Check if file exists\n    if os.path.exists(img_path):\n        valid_entries.append(row)\n    else:\n        print(f\"❌ Missing file: {img_path}\")\n\n# Save the cleaned CSV to Kaggle’s writable directory\ndf_cleaned = pd.DataFrame(valid_entries)\ncleaned_csv_path = \"/kaggle/working/labels_cleaned.csv\"  # Writable path\ndf_cleaned.to_csv(cleaned_csv_path, index=False)\n\nprint(f\"✅ Cleaned labels.csv saved at: {cleaned_csv_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T06:16:43.139219Z","iopub.execute_input":"2025-02-13T06:16:43.139563Z","iopub.status.idle":"2025-02-13T06:17:54.553133Z","shell.execute_reply.started":"2025-02-13T06:16:43.139538Z","shell.execute_reply":"2025-02-13T06:17:54.552226Z"}},"outputs":[{"name":"stdout","text":"✅ Cleaned labels.csv saved at: /kaggle/working/labels_cleaned.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport timm\nimport optuna\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Kaggle Paths\nINPUT_PATH = \"/kaggle/input/dog-vs-not-dog/Dog vs Not-Dog\"\nWORKING_PATH = \"/kaggle/working\"  # Writable directory\n\n# ================================\n# 1. Dataset Loading & Preprocessing\n# ================================\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Custom Dataset Class\nclass DogNotDogDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.data = pd.read_csv(csv_file, usecols=[\"filename\", \"label\"])\n        self.root_dir = root_dir\n        self.transform = transform\n\n        # Remove rows where image files are missing\n        valid_entries = []\n        for idx in range(len(self.data)):\n            img_name = str(self.data.iloc[idx, 0])\n            label = self.data.iloc[idx, 1]\n            img_path = os.path.join(self.root_dir, \"dog\" if label == \"dog\" else \"other\", img_name)\n            if os.path.exists(img_path):\n                valid_entries.append((img_name, label))\n\n        self.data = pd.DataFrame(valid_entries, columns=[\"filename\", \"label\"])\n        print(f\"✅ Final Dataset Loaded: {len(self.data)} samples\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = str(self.data.iloc[idx, 0])\n        label = self.data.iloc[idx, 1]\n        img_path = os.path.join(self.root_dir, \"dog\" if label == \"dog\" else \"other\", img_name)\n\n        if not os.path.exists(img_path):\n            return None\n\n        image = Image.open(img_path).convert(\"RGB\")\n        label = 1 if label == \"dog\" else 0\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Load and clean dataset\ncsv_file = os.path.join(INPUT_PATH, \"labels.csv\")\ndf = pd.read_csv(csv_file)\n\n# Remove missing files\nvalid_entries = []\nfor _, row in df.iterrows():\n    img_name = str(row[\"filename\"])\n    label = row[\"label\"]\n    img_path = os.path.join(INPUT_PATH, \"dog\" if label == \"dog\" else \"other\", img_name)\n    \n    if os.path.exists(img_path):\n        valid_entries.append(row)\n\n# Save cleaned CSV to Kaggle’s writable directory\ndf_cleaned = pd.DataFrame(valid_entries)\ncleaned_csv_path = os.path.join(WORKING_PATH, \"labels_cleaned.csv\")\ndf_cleaned.to_csv(cleaned_csv_path, index=False)\nprint(f\"✅ Cleaned labels.csv saved at: {cleaned_csv_path}\")\n\n# Load dataset with cleaned CSV\ndataset = DogNotDogDataset(csv_file=cleaned_csv_path, root_dir=INPUT_PATH, transform=transform)\n\n# Split into Training and Validation (80%-20%)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Custom Collate Function to Remove None Values\ndef collate_fn(batch):\n    batch = [sample for sample in batch if sample is not None]\n    if len(batch) == 0:\n        return None\n    images, labels = zip(*batch)\n    images = torch.stack(images)\n    labels = torch.tensor(labels, dtype=torch.long)\n    return images, labels\n\n# ================================\n# 2. Hyperparameter Optimization with Optuna\n# ================================\ndef objective(trial):\n    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n\n    print(f\"🔍 Running Trial {trial.number}: batch_size={batch_size}, lr={lr}\")\n\n    # Data Loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n\n    # Load BinaryViT Model\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = timm.create_model(\"deit_small_patch16_224\", pretrained=True, num_classes=2)\n    model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n\n    # Training Loop (Reduced Epochs for Speed)\n    num_epochs = 3\n    for epoch in range(num_epochs):\n        model.train()\n        for batch in train_loader:\n            if batch is None:\n                continue\n            images, labels = batch\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n    # Validation Loop\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            if batch is None:\n                continue\n            images, labels = batch\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n    accuracy = correct / total if total > 0 else 0.0\n    print(f\"✅ Trial {trial.number} Completed! Accuracy: {accuracy:.4f}\")\n    return accuracy\n\n# Run Optuna\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=5)\n\n# Best Hyperparameters\nbest_params = study.best_params\nprint(f\"✅ Best Hyperparameters: {best_params}\")\n\n# ================================\n# 3. Training with Best Parameters\n# ================================\nbatch_size = best_params[\"batch_size\"]\nlr = best_params[\"lr\"]\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n\n# Load Model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = timm.create_model(\"deit_small_patch16_224\", pretrained=True, num_classes=2)\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n\n# Training Loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        if batch is None:\n            continue\n        images, labels = batch\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n\n# Save the Best Model\nmodel_save_path = os.path.join(WORKING_PATH, \"binaryvit_dog_vs_not_dog_best.pth\")\ntorch.save(model.state_dict(), model_save_path)\n\nprint(f\"✅ Model training complete. Saved best model at {model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T06:22:55.486549Z","iopub.execute_input":"2025-02-13T06:22:55.486905Z","iopub.status.idle":"2025-02-13T07:55:18.496258Z","shell.execute_reply.started":"2025-02-13T06:22:55.486881Z","shell.execute_reply":"2025-02-13T07:55:18.495189Z"}},"outputs":[{"name":"stdout","text":"✅ Cleaned labels.csv saved at: /kaggle/working/labels_cleaned.csv\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-13 06:23:24,341] A new study created in memory with name: no-name-a9886f84-f11c-4167-bcaf-e05e1b9ea718\n","output_type":"stream"},{"name":"stdout","text":"✅ Final Dataset Loaded: 25124 samples\n🔍 Running Trial 0: batch_size=16, lr=0.0008774212834049186\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-13 06:33:58,115] Trial 0 finished with value: 0.6963184079601991 and parameters: {'batch_size': 16, 'lr': 0.0008774212834049186}. Best is trial 0 with value: 0.6963184079601991.\n","output_type":"stream"},{"name":"stdout","text":"✅ Trial 0 Completed! Accuracy: 0.6963\n🔍 Running Trial 1: batch_size=8, lr=0.00042282855726027166\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-13 06:45:28,275] Trial 1 finished with value: 0.8809950248756219 and parameters: {'batch_size': 8, 'lr': 0.00042282855726027166}. Best is trial 1 with value: 0.8809950248756219.\n","output_type":"stream"},{"name":"stdout","text":"✅ Trial 1 Completed! Accuracy: 0.8810\n🔍 Running Trial 2: batch_size=16, lr=0.0003882770276722218\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-13 06:56:06,342] Trial 2 finished with value: 0.9076616915422886 and parameters: {'batch_size': 16, 'lr': 0.0003882770276722218}. Best is trial 2 with value: 0.9076616915422886.\n","output_type":"stream"},{"name":"stdout","text":"✅ Trial 2 Completed! Accuracy: 0.9077\n🔍 Running Trial 3: batch_size=16, lr=0.000795150232016969\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-13 07:06:41,854] Trial 3 finished with value: 0.6875621890547263 and parameters: {'batch_size': 16, 'lr': 0.000795150232016969}. Best is trial 2 with value: 0.9076616915422886.\n","output_type":"stream"},{"name":"stdout","text":"✅ Trial 3 Completed! Accuracy: 0.6876\n🔍 Running Trial 4: batch_size=8, lr=3.826427643717009e-05\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-13 07:18:12,752] Trial 4 finished with value: 0.9872636815920398 and parameters: {'batch_size': 8, 'lr': 3.826427643717009e-05}. Best is trial 4 with value: 0.9872636815920398.\n","output_type":"stream"},{"name":"stdout","text":"✅ Trial 4 Completed! Accuracy: 0.9873\n✅ Best Hyperparameters: {'batch_size': 8, 'lr': 3.826427643717009e-05}\nEpoch 1, Loss: 0.0367362686712633\nEpoch 2, Loss: 0.0156353317110647\nEpoch 3, Loss: 0.013241967586930688\nEpoch 4, Loss: 0.012619685505431959\nEpoch 5, Loss: 0.01039992655272306\nEpoch 6, Loss: 0.008365249549175727\nEpoch 7, Loss: 0.011144134200818064\nEpoch 8, Loss: 0.005563266708853934\nEpoch 9, Loss: 0.008737380463819847\nEpoch 10, Loss: 0.008363364037403214\n✅ Model training complete. Saved best model at /kaggle/working/binaryvit_dog_vs_not_dog_best.pth\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport timm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\n\n# ================================\n# 1. Load Trained Model\n# ================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define model path (Kaggle working directory)\nmodel_path = \"/kaggle/working/binaryvit_dog_vs_not_dog_best.pth\"\n\n# Load the BinaryViT model with fixed security warning\nmodel = timm.create_model(\"deit_small_patch16_224\", pretrained=False, num_classes=2)\nmodel.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\nmodel.to(device)\nmodel.eval()\n\nprint(\"✅ Model loaded successfully from:\", model_path)\n\n# ================================\n# 2. Define Preprocessing Function\n# ================================\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# ================================\n# 3. Perform Inference on a Test Image\n# ================================\ntest_img_path = \"/kaggle/input/a/other/default/1/sample.jpg\"  # Updated Path\n\nif not os.path.exists(test_img_path):\n    raise FileNotFoundError(f\"❌ Test image not found: {test_img_path}\")\n\n# Load and preprocess image\nimage = Image.open(test_img_path).convert(\"RGB\")\nimage_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n\n# Get Prediction\nwith torch.no_grad():\n    output = model(image_tensor)\n    predicted_class = torch.argmax(output, dim=1).item()\n    label = \"Dog\" if predicted_class == 1 else \"Not Dog\"\n\nprint(f\"✅ Predicted Class: {label}\")\n\n# Save Prediction Result\nresult_path = \"/kaggle/working/prediction.txt\"\nwith open(result_path, \"w\") as f:\n    f.write(f\"Predicted Class: {label}\\n\")\nprint(f\"✅ Prediction result saved at: {result_path}\")\n\n# ================================\n# 4. Extract and Visualize Attention Map (Fixed)\n# ================================\nattention_maps = []\n\ndef hook_fn(module, input, output):\n    \"\"\"Hook function to extract attention maps from self-attention layers.\"\"\"\n    attention_maps.append(output.detach().cpu())  # Store raw attention weights\n\n# Find the last self-attention layer and register a hook\nfor name, module in model.named_modules():\n    if \"attn.proj\" in name:  # Extract from the attention projection layer\n        module.register_forward_hook(hook_fn)\n\nwith torch.no_grad():\n    _ = model(image_tensor)  # Forward pass to trigger hook\n\nif attention_maps:\n    attn = attention_maps[-1]  # Extracted attention tensor\n    attn = attn.mean(dim=1).squeeze(0).numpy()  # Average over heads\n\n    # Reshape attention map correctly (DeiT uses 14x14 patches for 224x224 input)\n    num_patches = 14 * 14  # 196 patches\n    attn = attn[:num_patches]  # Select patch tokens, excluding CLS token\n    attn = attn.reshape(14, 14)  # Convert to 2D grid\n\n    # ================================\n    # 5. Visualize Attention Map\n    # ================================\n    def visualize_attention(image, attention):\n        \"\"\"Plots the attention map overlaid on the image.\"\"\"\n        plt.figure(figsize=(8, 8))\n        sns.heatmap(attention, cmap=\"viridis\", alpha=0.5, square=True)\n        \n        # Resize image to match attention map shape\n        resized_image = image.permute(1, 2, 0).cpu().numpy()\n        plt.imshow(resized_image, alpha=0.6)\n        \n        plt.axis(\"off\")\n        plt.title(\"Attention Map\")\n\n        attention_map_path = \"/kaggle/working/attention_map.png\"\n        plt.savefig(attention_map_path)\n        print(f\"✅ Attention Map saved at: {attention_map_path}\")\n\n    visualize_attention(image_tensor.squeeze(), attn)\n\nelse:\n    print(\"❌ Attention map not available in this model.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T08:09:59.727731Z","iopub.execute_input":"2025-02-13T08:09:59.728046Z","iopub.status.idle":"2025-02-13T08:10:00.697002Z","shell.execute_reply.started":"2025-02-13T08:09:59.728024Z","shell.execute_reply":"2025-02-13T08:10:00.696313Z"}},"outputs":[{"name":"stdout","text":"✅ Model loaded successfully from: /kaggle/working/binaryvit_dog_vs_not_dog_best.pth\n✅ Predicted Class: Dog\n✅ Prediction result saved at: /kaggle/working/prediction.txt\n✅ Attention Map saved at: /kaggle/working/attention_map.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":9}]}