{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4969890,"sourceType":"datasetVersion","datasetId":2882514},{"sourceId":257181,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":219835,"modelId":241594}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Define dataset paths\ndataset_path = \"/kaggle/input/dog-vs-not-dog/Dog vs Not-Dog\"\ncsv_path = os.path.join(dataset_path, \"labels.csv\")\n\n# Load the CSV file\ndf = pd.read_csv(csv_path)\n\n# Create a new list for valid entries\nvalid_entries = []\nfor _, row in df.iterrows():\n    img_name = row[\"filename\"]\n    label = row[\"label\"]\n    \n    # Construct the expected file path\n    img_path = os.path.join(dataset_path, \"dog\" if label == \"dog\" else \"other\", img_name)\n\n    # Check if file exists\n    if os.path.exists(img_path):\n        valid_entries.append(row)\n    else:\n        print(f\"❌ Missing file: {img_path}\")\n\n# Save the cleaned CSV to Kaggle’s writable directory\ndf_cleaned = pd.DataFrame(valid_entries)\ncleaned_csv_path = \"/kaggle/working/labels_cleaned.csv\"  # Writable path\ndf_cleaned.to_csv(cleaned_csv_path, index=False)\n\nprint(f\"✅ Cleaned labels.csv saved at: {cleaned_csv_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T08:57:36.909529Z","iopub.execute_input":"2025-02-13T08:57:36.909869Z","iopub.status.idle":"2025-02-13T08:57:55.035936Z","shell.execute_reply.started":"2025-02-13T08:57:36.909845Z","shell.execute_reply":"2025-02-13T08:57:55.035047Z"}},"outputs":[{"name":"stdout","text":"✅ Cleaned labels.csv saved at: /kaggle/working/labels_cleaned.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport timm\nimport optuna\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Kaggle Paths\nINPUT_PATH = \"/kaggle/input/dog-vs-not-dog/Dog vs Not-Dog\"\nWORKING_PATH = \"/kaggle/working\"  # Writable directory\n\n# ================================\n# 1. Dataset Loading & Preprocessing\n# ================================\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Custom Dataset Class\nclass DogNotDogDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.data = pd.read_csv(csv_file, usecols=[\"filename\", \"label\"])\n        self.root_dir = root_dir\n        self.transform = transform\n\n        # Remove rows where image files are missing\n        valid_entries = []\n        for idx in range(len(self.data)):\n            img_name = str(self.data.iloc[idx, 0])\n            label = self.data.iloc[idx, 1]\n            img_path = os.path.join(self.root_dir, \"dog\" if label == \"dog\" else \"other\", img_name)\n            if os.path.exists(img_path):\n                valid_entries.append((img_name, label))\n\n        self.data = pd.DataFrame(valid_entries, columns=[\"filename\", \"label\"])\n        print(f\"✅ Final Dataset Loaded: {len(self.data)} samples\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = str(self.data.iloc[idx, 0])\n        label = self.data.iloc[idx, 1]\n        img_path = os.path.join(self.root_dir, \"dog\" if label == \"dog\" else \"other\", img_name)\n\n        if not os.path.exists(img_path):\n            return None\n\n        image = Image.open(img_path).convert(\"RGB\")\n        label = 1 if label == \"dog\" else 0\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Load and clean dataset\ncsv_file = os.path.join(INPUT_PATH, \"labels.csv\")\ndf = pd.read_csv(csv_file)\n\n# Remove missing files\nvalid_entries = []\nfor _, row in df.iterrows():\n    img_name = str(row[\"filename\"])\n    label = row[\"label\"]\n    img_path = os.path.join(INPUT_PATH, \"dog\" if label == \"dog\" else \"other\", img_name)\n    \n    if os.path.exists(img_path):\n        valid_entries.append(row)\n\n# Save cleaned CSV to Kaggle’s writable directory\ndf_cleaned = pd.DataFrame(valid_entries)\ncleaned_csv_path = os.path.join(WORKING_PATH, \"labels_cleaned.csv\")\ndf_cleaned.to_csv(cleaned_csv_path, index=False)\nprint(f\"✅ Cleaned labels.csv saved at: {cleaned_csv_path}\")\n\n# Load dataset with cleaned CSV\ndataset = DogNotDogDataset(csv_file=cleaned_csv_path, root_dir=INPUT_PATH, transform=transform)\n\n# Split into Training and Validation (80%-20%)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Custom Collate Function to Remove None Values\ndef collate_fn(batch):\n    batch = [sample for sample in batch if sample is not None]\n    if len(batch) == 0:\n        return None\n    images, labels = zip(*batch)\n    images = torch.stack(images)\n    labels = torch.tensor(labels, dtype=torch.long)\n    return images, labels\n\n# ================================\n# 2. Hyperparameter Optimization with Optuna\n# ================================\ndef objective(trial):\n    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n\n    print(f\"🔍 Running Trial {trial.number}: batch_size={batch_size}, lr={lr}\")\n\n    # Data Loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n\n    # Load BinaryViT Model\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = timm.create_model(\"deit_small_patch16_224\", pretrained=True, num_classes=2)\n    model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n\n    # Training Loop (Reduced Epochs for Speed)\n    num_epochs = 3\n    for epoch in range(num_epochs):\n        model.train()\n        for batch in train_loader:\n            if batch is None:\n                continue\n            images, labels = batch\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n    # Validation Loop\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            if batch is None:\n                continue\n            images, labels = batch\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n    accuracy = correct / total if total > 0 else 0.0\n    print(f\"✅ Trial {trial.number} Completed! Accuracy: {accuracy:.4f}\")\n    return accuracy\n\n# Run Optuna\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=5)\n\n# Best Hyperparameters\nbest_params = study.best_params\nprint(f\"✅ Best Hyperparameters: {best_params}\")\n\n# ================================\n# 3. Training with Best Parameters\n# ================================\nbatch_size = best_params[\"batch_size\"]\nlr = best_params[\"lr\"]\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n\n# Load Model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = timm.create_model(\"deit_small_patch16_224\", pretrained=True, num_classes=2)\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n\n# Training Loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for batch in train_loader:\n        if batch is None:\n            continue\n        images, labels = batch\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n\n# Save the Best Model\nmodel_save_path = os.path.join(WORKING_PATH, \"binaryvit_dog_vs_not_dog_best.pth\")\ntorch.save(model.state_dict(), model_save_path)\n\nprint(f\"✅ Model training complete. Saved best model at {model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T08:59:54.100148Z","iopub.execute_input":"2025-02-13T08:59:54.100543Z","iopub.status.idle":"2025-02-13T10:35:18.565963Z","shell.execute_reply.started":"2025-02-13T08:59:54.100512Z","shell.execute_reply":"2025-02-13T10:35:18.564739Z"}},"outputs":[{"name":"stdout","text":"✅ Cleaned labels.csv saved at: /kaggle/working/labels_cleaned.csv\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-13 09:00:35,172] A new study created in memory with name: no-name-694a7779-abd3-4af4-85d5-b4424e9b4631\n","output_type":"stream"},{"name":"stdout","text":"✅ Final Dataset Loaded: 25124 samples\n🔍 Running Trial 0: batch_size=16, lr=1.4533451342323778e-05\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/88.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3489b334e03040fd8851ae1d43ea9a9f"}},"metadata":{}},{"name":"stderr","text":"[I 2025-02-13 09:11:58,863] Trial 0 finished with value: 0.9968159203980099 and parameters: {'batch_size': 16, 'lr': 1.4533451342323778e-05}. Best is trial 0 with value: 0.9968159203980099.\n","output_type":"stream"},{"name":"stdout","text":"✅ Trial 0 Completed! Accuracy: 0.9968\n🔍 Running Trial 1: batch_size=16, lr=4.9385369463105545e-05\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-13 09:23:26,842] Trial 1 finished with value: 0.992636815920398 and parameters: {'batch_size': 16, 'lr': 4.9385369463105545e-05}. Best is trial 0 with value: 0.9968159203980099.\n","output_type":"stream"},{"name":"stdout","text":"✅ Trial 1 Completed! Accuracy: 0.9926\n🔍 Running Trial 2: batch_size=32, lr=0.0013459100779056902\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-13 09:35:20,167] Trial 2 finished with value: 0.6939303482587065 and parameters: {'batch_size': 32, 'lr': 0.0013459100779056902}. Best is trial 0 with value: 0.9968159203980099.\n","output_type":"stream"},{"name":"stdout","text":"✅ Trial 2 Completed! Accuracy: 0.6939\n🔍 Running Trial 3: batch_size=32, lr=3.696677668104352e-05\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-13 09:47:12,714] Trial 3 finished with value: 0.9962189054726368 and parameters: {'batch_size': 32, 'lr': 3.696677668104352e-05}. Best is trial 0 with value: 0.9968159203980099.\n","output_type":"stream"},{"name":"stdout","text":"✅ Trial 3 Completed! Accuracy: 0.9962\n🔍 Running Trial 4: batch_size=16, lr=0.0041399755059508435\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-13 09:58:30,056] Trial 4 finished with value: 0.644179104477612 and parameters: {'batch_size': 16, 'lr': 0.0041399755059508435}. Best is trial 0 with value: 0.9968159203980099.\n","output_type":"stream"},{"name":"stdout","text":"✅ Trial 4 Completed! Accuracy: 0.6442\n✅ Best Hyperparameters: {'batch_size': 16, 'lr': 1.4533451342323778e-05}\nEpoch 1, Loss: 0.04639758983664563\nEpoch 2, Loss: 0.003954699782041263\nEpoch 3, Loss: 0.0034797163995371537\nEpoch 4, Loss: 0.0005634697886940651\nEpoch 5, Loss: 0.003757412713511429\nEpoch 6, Loss: 0.00016684967623132324\nEpoch 7, Loss: 2.8397743538109886e-05\nEpoch 8, Loss: 1.4854252945335425e-05\nEpoch 9, Loss: 8.221345296807262e-06\nEpoch 10, Loss: 4.632846252103479e-06\n✅ Model training complete. Saved best model at /kaggle/working/binaryvit_dog_vs_not_dog_best.pth\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport timm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\n\n# ================================\n# 1. Load Trained Model\n# ================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define model path (Kaggle working directory)\nmodel_path = \"/kaggle/working/binaryvit_dog_vs_not_dog_best.pth\"\n\n# Load the BinaryViT model with fixed security warning\nmodel = timm.create_model(\"deit_small_patch16_224\", pretrained=False, num_classes=2)\nmodel.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\nmodel.to(device)\nmodel.eval()\n\nprint(\"✅ Model loaded successfully from:\", model_path)\n\n# ================================\n# 2. Define Preprocessing Function\n# ================================\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# ================================\n# 3. Perform Inference on a Test Image\n# ================================\ntest_img_path = \"/kaggle/input/a/other/default/1/sample.jpg\"  # Updated Path\n\nif not os.path.exists(test_img_path):\n    raise FileNotFoundError(f\"❌ Test image not found: {test_img_path}\")\n\n# Load and preprocess image\nimage = Image.open(test_img_path).convert(\"RGB\")\nimage_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n\n# Get Prediction\nwith torch.no_grad():\n    output = model(image_tensor)\n    predicted_class = torch.argmax(output, dim=1).item()\n    label = \"Dog\" if predicted_class == 1 else \"Not Dog\"\n\nprint(f\"✅ Predicted Class: {label}\")\n\n# Save Prediction Result\nresult_path = \"/kaggle/working/prediction.txt\"\nwith open(result_path, \"w\") as f:\n    f.write(f\"Predicted Class: {label}\\n\")\nprint(f\"✅ Prediction result saved at: {result_path}\")\n\n# ================================\n# 4. Extract and Visualize Attention Map (Fixed)\n# ================================\nattention_maps = []\n\ndef hook_fn(module, input, output):\n    \"\"\"Hook function to extract attention maps from self-attention layers.\"\"\"\n    attention_maps.append(output.detach().cpu())  # Store raw attention weights\n\n# Find the last self-attention layer and register a hook\nfor name, module in model.named_modules():\n    if \"attn.proj\" in name:  # Extract from the attention projection layer\n        module.register_forward_hook(hook_fn)\n\nwith torch.no_grad():\n    _ = model(image_tensor)  # Forward pass to trigger hook\n\nif attention_maps:\n    attn = attention_maps[-1]  # Extracted attention tensor\n    attn = attn.mean(dim=1).squeeze(0).numpy()  # Average over heads\n\n    # Reshape attention map correctly (DeiT uses 14x14 patches for 224x224 input)\n    num_patches = 14 * 14  # 196 patches\n    attn = attn[:num_patches]  # Select patch tokens, excluding CLS token\n    attn = attn.reshape(14, 14)  # Convert to 2D grid\n\n    # ================================\n    # 5. Visualize Attention Map\n    # ================================\n    def visualize_attention(image, attention):\n        \"\"\"Plots the attention map overlaid on the image.\"\"\"\n        plt.figure(figsize=(8, 8))\n        sns.heatmap(attention, cmap=\"viridis\", alpha=0.5, square=True)\n        \n        # Resize image to match attention map shape\n        resized_image = image.permute(1, 2, 0).cpu().numpy()\n        plt.imshow(resized_image, alpha=0.6)\n        \n        plt.axis(\"off\")\n        plt.title(\"Attention Map\")\n\n        attention_map_path = \"/kaggle/working/attention_map.png\"\n        plt.savefig(attention_map_path)\n        print(f\"✅ Attention Map saved at: {attention_map_path}\")\n\n    visualize_attention(image_tensor.squeeze(), attn)\n\nelse:\n    print(\"❌ Attention map not available in this model.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T10:35:41.473112Z","iopub.execute_input":"2025-02-13T10:35:41.473493Z","iopub.status.idle":"2025-02-13T10:35:43.170835Z","shell.execute_reply.started":"2025-02-13T10:35:41.473462Z","shell.execute_reply":"2025-02-13T10:35:43.169877Z"}},"outputs":[{"name":"stdout","text":"✅ Model loaded successfully from: /kaggle/working/binaryvit_dog_vs_not_dog_best.pth\n✅ Predicted Class: Dog\n✅ Prediction result saved at: /kaggle/working/prediction.txt\n✅ Attention Map saved at: /kaggle/working/attention_map.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":4}]}